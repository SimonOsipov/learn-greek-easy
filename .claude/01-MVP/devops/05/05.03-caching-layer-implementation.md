# 05.03 Caching Layer Implementation - Detailed Execution Plan

**Project**: Learn Greek Easy - DevOps
**Task**: Application Data Caching with Redis
**Parent**: [05-redis-configuration.md](./05-redis-configuration.md)
**Created**: 2025-12-03
**Status**: COMPLETE

---

## Table of Contents

1. [Objective](#objective)
2. [Problem Statement](#problem-statement)
3. [Prerequisites](#prerequisites)
4. [Technical Specifications](#technical-specifications)
5. [Implementation Steps](#implementation-steps)
6. [File Changes](#file-changes)
7. [Testing Requirements](#testing-requirements)
8. [Validation](#validation)
9. [Acceptance Criteria](#acceptance-criteria)
10. [Execution Checklist](#execution-checklist)

---

## Objective

Implement a Redis caching layer for frequently accessed data to:

1. **Reduce database load**: Cache read-heavy data like decks, cards, and user preferences
2. **Improve response times**: Sub-millisecond cache hits vs 10-50ms database queries
3. **Handle traffic spikes**: Serve cached content during high load periods
4. **Enable scalability**: Stateless application servers with shared cache

---

## Problem Statement

### Current Architecture

```
┌─────────────────────────────────────────────────────────────────┐
│                    Current Data Flow                             │
├─────────────────────────────────────────────────────────────────┤
│                                                                  │
│  Every Request                                                   │
│  ┌────────────────┐     ┌─────────────────────────┐            │
│  │ GET /decks     │────▶│ SELECT * FROM decks     │            │
│  └────────────────┘     │ (PostgreSQL)            │            │
│                         └─────────────────────────┘            │
│                                                                  │
│  Issues:                                                         │
│  - Same data queried repeatedly                                 │
│  - Database connection overhead per request                     │
│  - No protection against traffic spikes                         │
│  - Deck/card data changes infrequently                          │
│                                                                  │
└─────────────────────────────────────────────────────────────────┘
```

### Target Architecture

```
┌─────────────────────────────────────────────────────────────────┐
│                    Target Data Flow (Cache-Aside)               │
├─────────────────────────────────────────────────────────────────┤
│                                                                  │
│  Request Flow                                                    │
│  ┌────────────────┐                                             │
│  │ GET /decks     │                                             │
│  └───────┬────────┘                                             │
│          │                                                       │
│          ▼                                                       │
│  ┌────────────────┐     ┌─────────────────────────┐            │
│  │ Check Redis    │────▶│ GET cache:decks:list    │            │
│  │ Cache          │     └─────────────────────────┘            │
│  └───────┬────────┘                                             │
│          │                                                       │
│     ┌────┴────┐                                                 │
│     │         │                                                 │
│   [HIT]     [MISS]                                              │
│     │         │                                                 │
│     │         ▼                                                 │
│     │   ┌────────────────┐     ┌─────────────────────────┐    │
│     │   │ Query Database │────▶│ SELECT * FROM decks     │    │
│     │   └───────┬────────┘     └─────────────────────────┘    │
│     │           │                                               │
│     │           ▼                                               │
│     │   ┌────────────────┐     ┌─────────────────────────┐    │
│     │   │ Store in Redis │────▶│ SETEX cache:decks:list  │    │
│     │   │ with TTL       │     │ TTL: 5 minutes          │    │
│     │   └───────┬────────┘     └─────────────────────────┘    │
│     │           │                                               │
│     └─────┬─────┘                                               │
│           │                                                      │
│           ▼                                                      │
│   ┌────────────────┐                                            │
│   │ Return Data    │                                            │
│   └────────────────┘                                            │
│                                                                  │
└─────────────────────────────────────────────────────────────────┘
```

---

## Prerequisites

### Completed Dependencies

| Task | Status | Artifacts |
|------|--------|-----------|
| 05.01 Redis Container Setup | COMPLETE | Docker containers configured |
| 05.04 Redis Connection Pooling | COMPLETE | `src/core/redis.py` |
| 05.02 Session Storage | RECOMMENDED | SessionRepository pattern |

### Recommended First

Complete Task 05.02 (Session Storage) before this task to:
- Establish Redis repository patterns
- Test Redis integration thoroughly
- Have fallback mechanisms in place

### Verified Project Structure

```
learn-greek-easy-backend/
├── src/
│   ├── core/
│   │   ├── redis.py           # Redis client (EXISTS)
│   │   └── cache.py           # Cache service (TO CREATE)
│   ├── services/
│   │   ├── deck_service.py    # Deck service (TO MODIFY)
│   │   └── card_service.py    # Card service (TO MODIFY)
│   ├── api/
│   │   └── v1/
│   │       ├── decks.py       # Deck endpoints (TO MODIFY)
│   │       └── cards.py       # Card endpoints (TO MODIFY)
│   └── config.py              # Settings (TO MODIFY)
```

---

## Technical Specifications

### Cache Key Schema

```
Redis Key Namespace:
cache:
├── decks:                      # Deck caching
│   ├── list                    # All active decks (paginated)
│   ├── list:level:{level}      # Decks filtered by CEFR level
│   └── {deck_id}               # Individual deck with cards
├── cards:                      # Card caching
│   ├── deck:{deck_id}          # All cards in a deck
│   └── {card_id}               # Individual card
├── user:                       # User-specific caching
│   ├── {user_id}:progress      # User's deck progress
│   ├── {user_id}:due_cards     # Cards due for review
│   └── {user_id}:stats         # User statistics
└── meta:                       # Cache metadata
    └── version                 # Cache version for invalidation
```

### TTL Strategy

| Cache Type | TTL | Reason |
|------------|-----|--------|
| Deck list | 5 minutes | Changes rarely, frequently accessed |
| Individual deck | 10 minutes | More stable, less frequently accessed |
| Cards by deck | 5 minutes | Medium frequency access |
| User progress | 1 minute | Changes with reviews |
| Due cards | 30 seconds | Must be fresh for study sessions |
| User stats | 2 minutes | Computed data, acceptable delay |

### Cache Invalidation Strategy

**Write-Through + TTL**:
1. On data modification, delete relevant cache keys
2. TTL provides automatic cleanup for stale data
3. Next read will repopulate cache

**Invalidation Patterns**:

```python
# Deck updated -> invalidate deck-related caches
async def invalidate_deck(deck_id: UUID):
    keys = [
        "cache:decks:list",
        "cache:decks:list:level:*",  # All level filters
        f"cache:decks:{deck_id}",
        f"cache:cards:deck:{deck_id}",
    ]
    await cache.delete_pattern(keys)

# Card updated -> invalidate card and parent deck caches
async def invalidate_card(card_id: UUID, deck_id: UUID):
    keys = [
        f"cache:cards:{card_id}",
        f"cache:cards:deck:{deck_id}",
        f"cache:decks:{deck_id}",
    ]
    await cache.delete_pattern(keys)

# User completed review -> invalidate user caches
async def invalidate_user_progress(user_id: UUID):
    keys = [
        f"cache:user:{user_id}:progress",
        f"cache:user:{user_id}:due_cards",
        f"cache:user:{user_id}:stats",
    ]
    await cache.delete_pattern(keys)
```

### Configuration

```python
# src/config.py additions
class Settings(BaseSettings):
    # Caching
    cache_enabled: bool = Field(
        default=True,
        description="Enable Redis caching",
    )
    cache_key_prefix: str = Field(
        default="cache",
        description="Redis key prefix for cached data",
    )
    cache_default_ttl: int = Field(
        default=300,
        description="Default cache TTL in seconds (5 minutes)",
    )
    cache_deck_list_ttl: int = Field(
        default=300,
        description="Deck list cache TTL in seconds",
    )
    cache_deck_detail_ttl: int = Field(
        default=600,
        description="Individual deck cache TTL in seconds",
    )
    cache_user_progress_ttl: int = Field(
        default=60,
        description="User progress cache TTL in seconds",
    )
    cache_due_cards_ttl: int = Field(
        default=30,
        description="Due cards cache TTL in seconds",
    )
```

---

## Implementation Steps

### Step 1: Create Cache Service

Create `src/core/cache.py` with a generic caching service:

```python
"""Redis caching service with serialization and TTL management."""

import json
import logging
from datetime import timedelta
from typing import Any, Callable, List, Optional, TypeVar, Union

from redis.asyncio import Redis

from src.config import settings
from src.core.redis import get_redis

logger = logging.getLogger(__name__)

T = TypeVar("T")


class CacheService:
    """Generic caching service using Redis."""

    def __init__(self, redis: Optional[Redis] = None):
        """Initialize cache service.

        Args:
            redis: Optional Redis client. Uses global client if not provided.
        """
        self._redis = redis

    @property
    def redis(self) -> Optional[Redis]:
        """Get Redis client."""
        return self._redis or get_redis()

    @property
    def enabled(self) -> bool:
        """Check if caching is enabled."""
        return settings.cache_enabled and self.redis is not None

    def _key(self, *parts: str) -> str:
        """Build a cache key from parts."""
        return f"{settings.cache_key_prefix}:" + ":".join(parts)

    async def get(self, key: str) -> Optional[Any]:
        """Get value from cache.

        Args:
            key: Cache key (without prefix)

        Returns:
            Deserialized value or None if not found
        """
        if not self.enabled:
            return None

        full_key = self._key(key)
        try:
            data = await self.redis.get(full_key)
            if data:
                logger.debug(f"Cache HIT: {full_key}")
                return json.loads(data)
            logger.debug(f"Cache MISS: {full_key}")
            return None
        except Exception as e:
            logger.warning(f"Cache get error for {full_key}: {e}")
            return None

    async def set(
        self,
        key: str,
        value: Any,
        ttl: Optional[int] = None,
    ) -> bool:
        """Set value in cache.

        Args:
            key: Cache key (without prefix)
            value: Value to cache (must be JSON serializable)
            ttl: TTL in seconds (uses default if not provided)

        Returns:
            True if successful, False otherwise
        """
        if not self.enabled:
            return False

        full_key = self._key(key)
        cache_ttl = ttl or settings.cache_default_ttl

        try:
            serialized = json.dumps(value, default=str)
            await self.redis.setex(full_key, cache_ttl, serialized)
            logger.debug(f"Cache SET: {full_key} (TTL: {cache_ttl}s)")
            return True
        except Exception as e:
            logger.warning(f"Cache set error for {full_key}: {e}")
            return False

    async def delete(self, key: str) -> bool:
        """Delete a specific cache key.

        Args:
            key: Cache key (without prefix)

        Returns:
            True if key was deleted, False otherwise
        """
        if not self.enabled:
            return False

        full_key = self._key(key)
        try:
            result = await self.redis.delete(full_key)
            if result:
                logger.debug(f"Cache DELETE: {full_key}")
            return result > 0
        except Exception as e:
            logger.warning(f"Cache delete error for {full_key}: {e}")
            return False

    async def delete_pattern(self, pattern: str) -> int:
        """Delete all keys matching a pattern.

        Args:
            pattern: Key pattern (e.g., "decks:*")

        Returns:
            Number of keys deleted
        """
        if not self.enabled:
            return 0

        full_pattern = self._key(pattern)
        try:
            keys = []
            async for key in self.redis.scan_iter(match=full_pattern):
                keys.append(key)

            if keys:
                deleted = await self.redis.delete(*keys)
                logger.debug(f"Cache DELETE pattern {full_pattern}: {deleted} keys")
                return deleted
            return 0
        except Exception as e:
            logger.warning(f"Cache delete pattern error for {full_pattern}: {e}")
            return 0

    async def get_or_set(
        self,
        key: str,
        factory: Callable[[], Any],
        ttl: Optional[int] = None,
    ) -> Optional[Any]:
        """Get from cache or compute and store.

        This is the primary method for cache-aside pattern.

        Args:
            key: Cache key (without prefix)
            factory: Async function to compute value if not cached
            ttl: TTL in seconds (uses default if not provided)

        Returns:
            Cached or computed value
        """
        # Try cache first
        cached = await self.get(key)
        if cached is not None:
            return cached

        # Cache miss - compute value
        try:
            if asyncio.iscoroutinefunction(factory):
                value = await factory()
            else:
                value = factory()

            # Store in cache
            await self.set(key, value, ttl)
            return value

        except Exception as e:
            logger.error(f"Cache factory error for {key}: {e}")
            raise

    async def invalidate_deck(self, deck_id: str) -> int:
        """Invalidate all caches related to a deck.

        Args:
            deck_id: Deck UUID string

        Returns:
            Number of keys invalidated
        """
        count = 0
        count += await self.delete("decks:list")
        count += await self.delete_pattern("decks:list:level:*")
        count += await self.delete(f"decks:{deck_id}")
        count += await self.delete(f"cards:deck:{deck_id}")
        logger.info(f"Invalidated {count} cache keys for deck {deck_id}")
        return count

    async def invalidate_card(self, card_id: str, deck_id: str) -> int:
        """Invalidate all caches related to a card.

        Args:
            card_id: Card UUID string
            deck_id: Parent deck UUID string

        Returns:
            Number of keys invalidated
        """
        count = 0
        count += await self.delete(f"cards:{card_id}")
        count += await self.delete(f"cards:deck:{deck_id}")
        count += await self.delete(f"decks:{deck_id}")
        logger.info(f"Invalidated {count} cache keys for card {card_id}")
        return count

    async def invalidate_user_progress(self, user_id: str) -> int:
        """Invalidate all caches related to user progress.

        Args:
            user_id: User UUID string

        Returns:
            Number of keys invalidated
        """
        count = await self.delete_pattern(f"user:{user_id}:*")
        logger.info(f"Invalidated {count} cache keys for user {user_id}")
        return count


# Global cache instance
import asyncio

_cache_service: Optional[CacheService] = None


def get_cache() -> CacheService:
    """Get the global cache service instance."""
    global _cache_service
    if _cache_service is None:
        _cache_service = CacheService()
    return _cache_service
```

### Step 2: Create Cache Decorator

Add a decorator for easy caching of service methods:

```python
# src/core/cache.py (continued)

from functools import wraps
from typing import Callable, TypeVar

F = TypeVar("F", bound=Callable[..., Any])


def cached(
    key_template: str,
    ttl: Optional[int] = None,
    key_builder: Optional[Callable[..., str]] = None,
):
    """Decorator for caching async function results.

    Args:
        key_template: Cache key template with {param} placeholders
        ttl: Cache TTL in seconds
        key_builder: Optional function to build cache key from args/kwargs

    Example:
        @cached("decks:{deck_id}", ttl=600)
        async def get_deck(deck_id: UUID) -> Deck:
            ...

        @cached("decks:list:level:{level}")
        async def list_decks(level: Optional[str] = None) -> List[Deck]:
            ...
    """
    def decorator(func: F) -> F:
        @wraps(func)
        async def wrapper(*args, **kwargs):
            cache = get_cache()

            # Build cache key
            if key_builder:
                cache_key = key_builder(*args, **kwargs)
            else:
                # Extract key params from kwargs or function signature
                import inspect
                sig = inspect.signature(func)
                bound = sig.bind(*args, **kwargs)
                bound.apply_defaults()
                cache_key = key_template.format(**bound.arguments)

            # Try cache
            cached_value = await cache.get(cache_key)
            if cached_value is not None:
                return cached_value

            # Execute function
            result = await func(*args, **kwargs)

            # Cache result
            if result is not None:
                await cache.set(cache_key, result, ttl)

            return result

        return wrapper
    return decorator
```

### Step 3: Add Caching to Deck Service

Example integration with a deck service:

```python
# src/services/deck_service.py

from typing import List, Optional
from uuid import UUID

from src.core.cache import get_cache, cached
from src.db.models import Deck, DeckLevel
from src.schemas.deck import DeckResponse, DeckListResponse


class DeckService:
    def __init__(self, db: AsyncSession):
        self.db = db
        self.cache = get_cache()

    @cached("decks:list", ttl=settings.cache_deck_list_ttl)
    async def list_decks(
        self,
        level: Optional[DeckLevel] = None,
        include_inactive: bool = False,
    ) -> List[DeckResponse]:
        """List all decks, optionally filtered by level."""
        # Build query
        query = select(Deck)

        if not include_inactive:
            query = query.where(Deck.is_active == True)

        if level:
            query = query.where(Deck.level == level)

        query = query.order_by(Deck.level, Deck.name)

        # Execute
        result = await self.db.execute(query)
        decks = result.scalars().all()

        # Convert to response schema
        return [DeckResponse.model_validate(d) for d in decks]

    async def get_deck(self, deck_id: UUID) -> Optional[DeckResponse]:
        """Get a single deck by ID with caching."""
        cache_key = f"decks:{deck_id}"

        # Check cache
        cached = await self.cache.get(cache_key)
        if cached:
            return DeckResponse(**cached)

        # Query database
        result = await self.db.execute(
            select(Deck).where(Deck.id == deck_id)
        )
        deck = result.scalar_one_or_none()

        if not deck:
            return None

        response = DeckResponse.model_validate(deck)

        # Cache result
        await self.cache.set(
            cache_key,
            response.model_dump(),
            settings.cache_deck_detail_ttl,
        )

        return response

    async def get_deck_with_cards(self, deck_id: UUID) -> Optional[DeckDetailResponse]:
        """Get deck with all its cards."""
        cache_key = f"decks:{deck_id}:full"

        # Check cache
        cached = await self.cache.get(cache_key)
        if cached:
            return DeckDetailResponse(**cached)

        # Query database with eager loading
        result = await self.db.execute(
            select(Deck)
            .options(selectinload(Deck.cards))
            .where(Deck.id == deck_id)
        )
        deck = result.scalar_one_or_none()

        if not deck:
            return None

        response = DeckDetailResponse.model_validate(deck)

        # Cache result
        await self.cache.set(
            cache_key,
            response.model_dump(),
            settings.cache_deck_detail_ttl,
        )

        return response

    async def update_deck(self, deck_id: UUID, data: DeckUpdate) -> Optional[DeckResponse]:
        """Update a deck and invalidate cache."""
        # ... update logic ...

        # Invalidate caches
        await self.cache.invalidate_deck(str(deck_id))

        return updated_deck

    async def delete_deck(self, deck_id: UUID) -> bool:
        """Delete a deck and invalidate cache."""
        # ... delete logic ...

        # Invalidate caches
        await self.cache.invalidate_deck(str(deck_id))

        return True
```

### Step 4: Add Caching to Card Service

```python
# src/services/card_service.py

class CardService:
    def __init__(self, db: AsyncSession):
        self.db = db
        self.cache = get_cache()

    async def get_cards_by_deck(self, deck_id: UUID) -> List[CardResponse]:
        """Get all cards in a deck with caching."""
        cache_key = f"cards:deck:{deck_id}"

        # Check cache
        cached = await self.cache.get(cache_key)
        if cached:
            return [CardResponse(**c) for c in cached]

        # Query database
        result = await self.db.execute(
            select(Card)
            .where(Card.deck_id == deck_id)
            .order_by(Card.order_index)
        )
        cards = result.scalars().all()

        responses = [CardResponse.model_validate(c) for c in cards]

        # Cache result
        await self.cache.set(
            cache_key,
            [r.model_dump() for r in responses],
            settings.cache_deck_list_ttl,
        )

        return responses

    async def update_card(self, card_id: UUID, data: CardUpdate) -> Optional[CardResponse]:
        """Update a card and invalidate cache."""
        # ... update logic ...

        # Invalidate caches
        await self.cache.invalidate_card(str(card_id), str(card.deck_id))

        return updated_card
```

### Step 5: Add User Progress Caching

```python
# src/services/progress_service.py

class ProgressService:
    def __init__(self, db: AsyncSession):
        self.db = db
        self.cache = get_cache()

    async def get_due_cards(self, user_id: UUID) -> List[CardForReviewResponse]:
        """Get cards due for review with short-lived cache."""
        cache_key = f"user:{user_id}:due_cards"

        # Very short TTL for due cards
        cached = await self.cache.get(cache_key)
        if cached:
            return [CardForReviewResponse(**c) for c in cached]

        # Query due cards
        today = date.today()
        result = await self.db.execute(
            select(CardStatistics)
            .options(selectinload(CardStatistics.card))
            .where(
                CardStatistics.user_id == user_id,
                CardStatistics.next_review_date <= today,
            )
            .order_by(CardStatistics.next_review_date)
            .limit(50)  # Limit batch size
        )
        stats = result.scalars().all()

        responses = [CardForReviewResponse.from_statistics(s) for s in stats]

        # Short TTL cache
        await self.cache.set(
            cache_key,
            [r.model_dump() for r in responses],
            settings.cache_due_cards_ttl,  # 30 seconds
        )

        return responses

    async def record_review(self, user_id: UUID, card_id: UUID, rating: int) -> None:
        """Record a review and invalidate user progress cache."""
        # ... review logic ...

        # Invalidate user caches
        await self.cache.invalidate_user_progress(str(user_id))
```

### Step 6: Add Cache Metrics Endpoint (Optional)

```python
# src/api/admin.py

from src.core.cache import get_cache

@router.get("/admin/cache/stats", include_in_schema=False)
async def cache_stats():
    """Get cache statistics (admin only)."""
    redis = get_redis()
    if not redis:
        return {"status": "disabled"}

    info = await redis.info("stats")
    memory = await redis.info("memory")

    return {
        "status": "enabled",
        "hits": info.get("keyspace_hits", 0),
        "misses": info.get("keyspace_misses", 0),
        "hit_ratio": info.get("keyspace_hits", 0) / max(1, info.get("keyspace_hits", 0) + info.get("keyspace_misses", 0)),
        "memory_used_mb": memory.get("used_memory", 0) / 1024 / 1024,
        "keys": await redis.dbsize(),
    }
```

---

## File Changes

### Files to Create

| File | Purpose |
|------|---------|
| `src/core/cache.py` | Cache service and decorator |

### Files to Modify

| File | Changes |
|------|---------|
| `src/config.py` | Add cache configuration settings |
| `src/services/deck_service.py` | Add caching to deck operations |
| `src/services/card_service.py` | Add caching to card operations |
| `src/services/progress_service.py` | Add caching to progress operations |

### Files Unchanged

| File | Reason |
|------|--------|
| `src/core/redis.py` | Already complete |
| `src/db/models.py` | No changes needed |

---

## Testing Requirements

### Unit Tests

```python
# tests/unit/test_cache_service.py
import pytest
from unittest.mock import AsyncMock, patch
from uuid import uuid4

from src.core.cache import CacheService, cached


class TestCacheService:
    @pytest.fixture
    def mock_redis(self):
        redis = AsyncMock()
        return redis

    @pytest.fixture
    def cache(self, mock_redis):
        return CacheService(redis=mock_redis)

    async def test_get_cache_hit(self, cache, mock_redis):
        mock_redis.get.return_value = '{"name": "Test Deck"}'

        result = await cache.get("decks:123")

        assert result == {"name": "Test Deck"}
        mock_redis.get.assert_called_once()

    async def test_get_cache_miss(self, cache, mock_redis):
        mock_redis.get.return_value = None

        result = await cache.get("decks:123")

        assert result is None

    async def test_set_with_ttl(self, cache, mock_redis):
        await cache.set("decks:123", {"name": "Test"}, ttl=600)

        mock_redis.setex.assert_called_once()
        args = mock_redis.setex.call_args
        assert args[0][1] == 600  # TTL

    async def test_delete_pattern(self, cache, mock_redis):
        mock_redis.scan_iter.return_value = AsyncMock()
        mock_redis.scan_iter.return_value.__aiter__.return_value = iter([
            "cache:decks:list",
            "cache:decks:list:level:A1",
        ])
        mock_redis.delete.return_value = 2

        count = await cache.delete_pattern("decks:*")

        assert count == 2

    async def test_invalidate_deck(self, cache, mock_redis):
        mock_redis.delete.return_value = 1
        mock_redis.scan_iter.return_value = AsyncMock()
        mock_redis.scan_iter.return_value.__aiter__.return_value = iter([])

        count = await cache.invalidate_deck("deck-123")

        assert mock_redis.delete.call_count >= 2


class TestCachedDecorator:
    async def test_cached_returns_cached_value(self):
        with patch("src.core.cache.get_cache") as mock_get_cache:
            mock_cache = AsyncMock()
            mock_cache.get.return_value = {"cached": True}
            mock_get_cache.return_value = mock_cache

            @cached("test:{param}")
            async def test_func(param: str):
                return {"fresh": True}

            result = await test_func(param="value")

            assert result == {"cached": True}

    async def test_cached_computes_on_miss(self):
        with patch("src.core.cache.get_cache") as mock_get_cache:
            mock_cache = AsyncMock()
            mock_cache.get.return_value = None
            mock_get_cache.return_value = mock_cache

            @cached("test:{param}")
            async def test_func(param: str):
                return {"fresh": True}

            result = await test_func(param="value")

            assert result == {"fresh": True}
            mock_cache.set.assert_called_once()
```

### Integration Tests

```python
# tests/integration/test_cache_integration.py
import pytest
from uuid import uuid4

from src.core.cache import CacheService, get_cache
from src.core.redis import init_redis, close_redis, get_redis


@pytest.mark.integration
class TestCacheIntegration:
    @pytest.fixture(autouse=True)
    async def setup_redis(self):
        await init_redis()
        yield
        # Cleanup
        redis = get_redis()
        if redis:
            keys = await redis.keys("cache:test:*")
            if keys:
                await redis.delete(*keys)
        await close_redis()

    async def test_cache_roundtrip(self):
        cache = CacheService()

        # Set value
        await cache.set("test:key1", {"data": "value"}, ttl=60)

        # Get value
        result = await cache.get("test:key1")

        assert result == {"data": "value"}

    async def test_cache_ttl_expiry(self):
        cache = CacheService()

        # Set with 1 second TTL
        await cache.set("test:expiring", {"temp": True}, ttl=1)

        # Should exist immediately
        result1 = await cache.get("test:expiring")
        assert result1 is not None

        # Wait for expiry
        import asyncio
        await asyncio.sleep(1.5)

        # Should be expired
        result2 = await cache.get("test:expiring")
        assert result2 is None

    async def test_delete_pattern(self):
        cache = CacheService()

        # Set multiple keys
        await cache.set("test:pattern:a", "a")
        await cache.set("test:pattern:b", "b")
        await cache.set("test:other:c", "c")

        # Delete pattern
        deleted = await cache.delete_pattern("test:pattern:*")

        assert deleted == 2

        # Verify
        assert await cache.get("test:pattern:a") is None
        assert await cache.get("test:pattern:b") is None
        assert await cache.get("test:other:c") == "c"
```

### Service Integration Tests

```python
# tests/integration/test_deck_caching.py
@pytest.mark.integration
class TestDeckServiceCaching:
    async def test_list_decks_uses_cache(self, db_session, cache):
        service = DeckService(db_session)

        # First call - should hit database
        result1 = await service.list_decks()

        # Second call - should hit cache
        result2 = await service.list_decks()

        # Results should be equal
        assert result1 == result2

        # Verify cache was populated
        cached = await cache.get("decks:list")
        assert cached is not None

    async def test_update_invalidates_cache(self, db_session, cache, test_deck):
        service = DeckService(db_session)

        # Populate cache
        await service.get_deck(test_deck.id)
        assert await cache.get(f"decks:{test_deck.id}") is not None

        # Update deck
        await service.update_deck(test_deck.id, DeckUpdate(name="New Name"))

        # Cache should be invalidated
        assert await cache.get(f"decks:{test_deck.id}") is None
```

---

## Validation

### Pre-Flight Checklist

- [ ] Redis container is running and healthy
- [ ] Session storage (Task 05.02) is complete (recommended)
- [ ] Deck/Card API endpoints exist
- [ ] All existing tests pass

### Manual Validation Steps

```bash
# 1. Start services
cd /Users/samosipov/Downloads/learn-greek-easy && docker-compose -f docker-compose.dev.yml up -d

# 2. Clear Redis cache
docker exec learn-greek-redis-dev redis-cli FLUSHDB

# 3. Make first request (cache miss)
time curl -s http://localhost:8000/api/v1/decks | head -c 100
# Note the response time

# 4. Make second request (cache hit)
time curl -s http://localhost:8000/api/v1/decks | head -c 100
# Should be significantly faster

# 5. Verify cache key exists
docker exec learn-greek-redis-dev redis-cli KEYS "cache:decks:*"

# 6. Check cache content
docker exec learn-greek-redis-dev redis-cli GET "cache:decks:list"

# 7. Check TTL
docker exec learn-greek-redis-dev redis-cli TTL "cache:decks:list"

# 8. Verify cache invalidation (after update)
# Make a deck update via API
# Then verify cache was cleared
docker exec learn-greek-redis-dev redis-cli KEYS "cache:decks:*"

# 9. Run test suite
cd /Users/samosipov/Downloads/learn-greek-easy/learn-greek-easy-backend && \
/Users/samosipov/.local/bin/poetry run pytest tests/ -v -k "cache"
```

### Performance Validation

```python
# scripts/benchmark_cache.py
"""Benchmark cache performance."""

import asyncio
import time
from statistics import mean, stdev

async def benchmark():
    # Without cache
    times_no_cache = []
    for _ in range(10):
        start = time.perf_counter()
        await fetch_decks_from_db()
        times_no_cache.append((time.perf_counter() - start) * 1000)

    # With cache (warm)
    await cache.set("decks:list", decks_data)
    times_cache = []
    for _ in range(10):
        start = time.perf_counter()
        await cache.get("decks:list")
        times_cache.append((time.perf_counter() - start) * 1000)

    print(f"Database: {mean(times_no_cache):.2f}ms (std: {stdev(times_no_cache):.2f})")
    print(f"Cache:    {mean(times_cache):.2f}ms (std: {stdev(times_cache):.2f})")
    print(f"Speedup:  {mean(times_no_cache) / mean(times_cache):.1f}x")
```

---

## Acceptance Criteria

- [ ] CacheService provides get/set/delete operations
- [ ] Cache-aside pattern implemented for deck listing
- [ ] Cache-aside pattern implemented for individual decks
- [ ] Cache-aside pattern implemented for cards by deck
- [ ] User progress caching with short TTL
- [ ] Cache invalidation on deck/card updates
- [ ] Cache invalidation on user progress changes
- [ ] Graceful degradation when Redis unavailable
- [ ] All existing tests pass
- [ ] New cache unit tests pass
- [ ] New cache integration tests pass
- [ ] Performance improvement measurable (>10x for cache hits)
- [ ] TTLs configurable via environment variables

---

## Execution Checklist

### Phase 1: Core Implementation

1. [ ] Add cache configuration to `src/config.py`
2. [ ] Create `src/core/cache.py` with CacheService
3. [ ] Add `@cached` decorator
4. [ ] Write unit tests for CacheService

### Phase 2: Service Integration

5. [ ] Add caching to DeckService
6. [ ] Add caching to CardService
7. [ ] Add caching to ProgressService
8. [ ] Implement cache invalidation in update/delete methods

### Phase 3: Testing

9. [ ] Write integration tests for cache operations
10. [ ] Write integration tests for service caching
11. [ ] Run full test suite
12. [ ] Performance benchmarking

### Phase 4: Validation

13. [ ] Manual testing of cache behavior
14. [ ] Verify cache invalidation works correctly
15. [ ] Test Redis failure/recovery scenario
16. [ ] Verify TTL behavior

---

**Ready for executor mode.**
